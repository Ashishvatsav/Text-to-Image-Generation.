# Text-to-Image-Generation.
Text-to-Image Generation using Custom DCGAN

 Overview

This project focuses on generating images from natural language descriptions using a Custom Deep Convolutional Generative Adversarial Network (DCGAN). The goal is to explore how conditional GANs can be applied for text-to-image synthesis while experimenting with architectural modifications, dataset choice, and optimization strategies for improved stability and quality of generated images.

 Objectives
	‚Ä¢	Implement a custom DCGAN for text-to-image generation.
	‚Ä¢	Experiment with different dataset (Oxford 102 Flowers) instead of generic datasets like CIFAR or COCO.
	‚Ä¢	Enhance training stability with architectural modifications and regularization.
	‚Ä¢	Evaluate results using both quantitative metrics and visual quality.

 Dataset

The project uses the Oxford 102 Flowers dataset, which contains:
	‚Ä¢	8,189 images of flowers across 102 categories.
	‚Ä¢	Each image is paired with natural language descriptions (captions).
	‚Ä¢	Dataset is preprocessed by:
	‚Ä¢	Resizing and normalizing images.
	‚Ä¢	Tokenizing captions and converting them into word embeddings.

 Methodology

1. Text Preprocessing
	‚Ä¢	Tokenization of captions.
	‚Ä¢	Embedding generation (Word2Vec/GloVe).
	‚Ä¢	Alignment of text features with latent noise vectors.

2. Model Architecture

Generator:
	‚Ä¢	Input: concatenation of random noise vector (z=128) + text embedding.
	‚Ä¢	Layers: ConvTranspose2D, BatchNorm, Dropout, GELU activations.
	‚Ä¢	Output: Synthetic image conditioned on the caption.

Discriminator:
	‚Ä¢	Input: real/fake image + text embedding.
	‚Ä¢	Layers: Conv2D, Spectral Normalization, LeakyReLU.
	‚Ä¢	Output: Probability (real/fake) with respect to text.

3. Training Setup
	‚Ä¢	Optimizer: RMSProp instead of Adam (for smoother convergence).
	‚Ä¢	Loss function: Binary Cross Entropy with label smoothing.
	‚Ä¢	Stabilization techniques:
	‚Ä¢	Data augmentation (random flips, crops).
	‚Ä¢	Spectral Normalization.
	‚Ä¢	Gradual learning rate decay.

4. Hyperparameter Tuning
	‚Ä¢	Used Optuna for automated search.
	‚Ä¢	Tuned parameters: learning rate, batch size, embedding dimension, latent vector size.

üìä Results
	‚Ä¢	Generated images show clear alignment with textual descriptions.
	‚Ä¢	Visual examples:
	‚Ä¢	Input: ‚ÄúA purple flower with five petals‚Äù ‚Üí Generated image resembles correct petal shape and color.
	‚Ä¢	Input: ‚ÄúA yellow sunflower with a dark center‚Äù ‚Üí Generated image reflects distinct sunflower structure.
	‚Ä¢	Evaluation Metrics:
	‚Ä¢	FID (Fr√©chet Inception Distance) used to measure quality and diversity.
	‚Ä¢	Lower FID observed after applying spectral normalization and label smoothing.

‚öôÔ∏è Technologies Used
	‚Ä¢	Python 3.x
	‚Ä¢	PyTorch / Torchvision
	‚Ä¢	Numpy, Pandas
	‚Ä¢	Matplotlib, Seaborn
	‚Ä¢	Optuna (hyperparameter tuning)
	‚Ä¢	NLTK / SpaCy (text preprocessing)
